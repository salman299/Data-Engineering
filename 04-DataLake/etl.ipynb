{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "# Extrac static data\n",
    "shutil.unpack_archive('data/song-data.zip', 'data')\n",
    "shutil.unpack_archive('data/log-data.zip', 'data/log_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, TimestampType\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['DEFAULT']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['DEFAULT']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = \"data/\"\n",
    "output_data = \"\"\n",
    "\n",
    "# get filepath to song data file\n",
    "song_data = input_data+'song_data/*/*/*/*.json'\n",
    "\n",
    "# read song data fileS\n",
    "df = spark.read.json(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Process a song_data. \n",
    "    Reads a song_data using a spark session and create songs and artist tables from it.\n",
    "    Parameters:\n",
    "        spark (pyspark.sql.session.SparkSession): SparkSession\n",
    "        input_data (str): path of a input data\n",
    "        output_data (str): path of a output data\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data =  input_data+'song_data/*/*/*/*.json'\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.selectExpr('song_id', 'title', 'artist_id', 'year', 'duration').dropDuplicates()\n",
    "    songs_table.createOrReplaceTempView('songs')\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.parquet(os.path.join(output_data, 'songs'), 'overwrite', partitionBy=['year', 'artist_id'])\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    columns = ['artist_name as artist', 'artist_location as location', 'artist_latitude as latitude', 'artist_longitude as longitude']\n",
    "    artists_table = df.selectExpr('artist_id', *columns).dropDuplicates()\n",
    "    artists_table.createOrReplaceTempView('artists')\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.parquet(os.path.join(output_data, 'artists'), 'overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_song_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_data = input_data+'log_data/*.json'\n",
    "df = spark.read.json(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Process a log_data. \n",
    "    Reads a log_data using a spark session and create users, time, and songplays tables from it.\n",
    "    Parameters:\n",
    "        spark (pyspark.sql.session.SparkSession): SparkSession\n",
    "        input_data (str): path of a input data\n",
    "        output_data (str): path of a output data\n",
    "    \"\"\"\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data+'log_data/*.json'\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    df.withColumn('user_id', df.userId.cast(IntegerType()))\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "    # extract columns for users table\n",
    "    columns = ['userId as user_id', 'firstName as first_name', 'lastName as last_name', 'gender', 'level']\n",
    "    users_table = df.selectExpr(*columns).dropDuplicates()\n",
    "    \n",
    "    users_table.createOrReplaceTempView('users')\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.parquet(os.path.join(output_data, 'users'), 'overwrite')\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda ts: datetime.fromtimestamp(ts/1000).isoformat())\n",
    "    df = df.withColumn('start_time', get_timestamp('ts').cast(TimestampType()))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.select('start_time')\n",
    "    time_table = time_table.withColumn('hour', hour('start_time'))\n",
    "    time_table = time_table.withColumn('day', dayofmonth('start_time'))\n",
    "    time_table = time_table.withColumn('week', weekofyear('start_time'))\n",
    "    time_table = time_table.withColumn('month', month('start_time'))\n",
    "    time_table = time_table.withColumn('year', year('start_time'))\n",
    "    time_table = time_table.withColumn('weekday', dayofweek('start_time'))\n",
    "    \n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.parquet(os.path.join(output_data, 'time'), 'overwrite', partitionBy=['year', 'month'])\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.read.json(os.path.join(input_data, 'song_data/*/*/*/*.json'))\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table\n",
    "    df = df.join(song_df, song_df.title == df.song)\n",
    "    columns = ['ts', 'userId as user_id', 'level', 'song_id', 'artist_id', 'sessionId as session_id', \n",
    "               'location', 'userAgent as user_agent', 'year', \"month('start_time') as month\"]\n",
    "    songplays_table = df.selectExpr(*columns)\n",
    "    \n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.parquet(os.path.join(output_data, 'songplays'), 'overwrite', partitionBy=['year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'artists': No such file or directory\n",
      "rm: cannot remove 'songplays': No such file or directory\n",
      "rm: cannot remove 'time': No such file or directory\n",
      "rm: cannot remove 'users': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# To remove all parquet directories\n",
    "!rm -r artists\n",
    "!rm -r songplays\n",
    "!rm -r time\n",
    "!rm -r users\n",
    "!rm -r songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
