# Data Lake Project

Hello! this repository provides a solution to Project (**Project: Data Lake)** of the Data Engineering nano degree at [Udacity](https://www.udacity.com/course/data-engineer-nanodegree--nd027).

## **Project Instructions**

### **Prerequisite**

- Create an IAM role, Redshift cluster, and open a TCP connection to allow third-party applications to interact with the cluster.
- Update your credentials in `dwh.cfg`

### **Jupyter Notebook Testing**

- Optional: Run `etl.ipynb` Jupyter notebook to test the functions. We are using testing data in this notebook. The testing data is present on a root level `data/`

### **ETL Process**

- Run `python etl.py` This will extract data from all the .json files from s3 bucket and creates various tables from it.

## Dataset

### ****Song Dataset****

The first dataset is a subset of real data from the **[Million Song Dataset](http://millionsongdataset.com/).** Each file is in JSON format and contains metadata about a song and the artist of that song. Each JSON file contains information about artists and songs.

```
data/song_data/A/B/C/TRABCEI128F424C983.json
data/song_data/A/A/B/TRAABJL12903CDCF1A.json
```

Example:

```json
{
   "num_songs":1,
   "artist_id":"ARJIE2Y1187B994AB7",
   "artist_latitude":null,
   "artist_longitude":null,
   "artist_location":"",
   "artist_name":"Line Renaud",
   "song_id":"SOUPIRU12A6D4FA1E1",
   "title":"Der Kleine Dompfaff",
   "duration":152.92036,
   "year":0
}
```

### ****Song Dataset****

The second dataset consists of log files in JSON format generated by this **[event simulator](https://github.com/Interana/eventsim)**
 based on the songs in the dataset above.

```
data/log_data/2018/11/2018-11-12-events.json
data/log_data/2018/11/2018-11-13-events.json
```

Example:

```
{
   "artist":null,
   "auth":"Logged In",
   "firstName":"Walter",
   "gender":"M",
   "itemInSession":0,
   "lastName":"Frye",
   "length":null,
   "level":"free",
   "location":"San Francisco-Oakland-Hayward, CA",
   "method":"GET",
   "page":"Home",
   "registration":1540919166796.0,
   "sessionId":38,
   "song":null,
   "status":200,
   "ts":1541105830796,
   "userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"",
   "userId":"39"
}{
   "artist":null,
   "auth":"Logged In",
   "firstName":"Kaylee",
   "gender":"F",
   "itemInSession":0,
   "lastName":"Summers",
   "length":null,
   "level":"free",
   "location":"Phoenix-Mesa-Scottsdale, AZ",
   "method":"GET",
   "page":"Home",
   "registration":1540344794796.0,
   "sessionId":139,
   "song":null,
   "status":200,
   "ts":1541106106796,
   "userAgent":"\"Mozilla\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/35.0.1916.153 Safari\/537.36\"",
   "userId":"8"
}
```

## Star **Schema**

**Fact Table:**

- s**ongplays** - ( *****songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)*

**Dimension Tables**

- **users** - (*user_id, first_name, last_name, gender, level)*
- **songs** - (*song_id, title, artist_id, year, duration)*
- **artists** - (*artist_id, name, location, lattitude, longitude*)
- **time** - (*start_time, hour, day, week, month, year, weekday)*

### **Representation of Star Schema**

![Untitled](static/star_schema.png)

### **Why this structure?**

All of the data warehouse techniques could be applicable by using big data technologies i.e Spark. In this ETL pipeline, we have implemented a star schema. Using a schema-on-read functionality of Sparkify we don’t need any staging area to store the data and then process it. It makes our pipeline clean and easy to implement. Moreover, implementing a Star Schema have a lot of benefits. Now, most of the analytical queries are doable from just one table `songplays` and it’s also possible to get in-depth information by simple JOINS.